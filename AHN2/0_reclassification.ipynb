{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pdal\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reclassify AHN2 ground points\n",
    "The ground points have the field `raw_classification` set as 0 (never classified). It is necessary to set this to 2 (ground points) for the calculation of the pulse penetratio ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = pathlib.Path('/data/local/home/eecolidar_webdav/00_Data/ALS/Netherlands/ahn2/terrain')\n",
    "# input_path = pathlib.Path('/data/local/tmp')\n",
    "output_path = pathlib.Path('/data/local/home/eecolidar_webdav/01_Escience/ALS/Netherlands/ahn2/terrain_reclassified')\n",
    "# output_path = pathlib.Path('/data/local/tmp/reclassified')\n",
    "\n",
    "run = 'from_file' # 'all', 'from_file'\n",
    "filename = 'reclassification_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'from_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 30080 LAZ files\n",
      "Retrieve and reclassify: 45 LAZ files\n"
     ]
    }
   ],
   "source": [
    "files = [el for el in input_path.iterdir() if el.suffix == '.laz']\n",
    "print('Found: {} LAZ files'.format(len(files)))\n",
    "if run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        files_read = json.load(f)\n",
    "    files_read = [pathlib.Path(f) for f in files_read]\n",
    "    # check whether all files are available \n",
    "    assert all([f in files for f in files_read]), f'Some of the files in {filename} are not in remote dir'\n",
    "    files = files_read\n",
    "print('Retrieve and reclassify: {} LAZ files'.format(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://node1:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='/proxy/8787/status' target='_blank'>/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://145.100.59.123:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client('node1:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using PDAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_as_ground_points(input_file, output_file):\n",
    "    PDAL_pipeline_dict = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"tag\": \"ground_laz\",\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": input_file\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.assign\",\n",
    "                \"assignment\": \"Classification[:]=2\",\n",
    "                \"tag\": \"ground_classed\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.las\",\n",
    "                \"filename\": output_file,\n",
    "                \"forward\": [\"scale_x\", \"scale_y\", \"scale_z\"],\n",
    "                \"offset_x\": \"auto\",\n",
    "                \"offset_y\": \"auto\",\n",
    "                \"offset_z\": \"auto\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    PDAL_pipeline = pdal.Pipeline(json.dumps(PDAL_pipeline_dict))\n",
    "    PDAL_pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = [output_path/f.name.replace('.laz', '_reclassified.laz') for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = [client.submit(classify_as_ground_points,\n",
    "                         input_file.as_posix(),\n",
    "                         output_file.as_posix())\n",
    "           for input_file, output_file in zip(files, out_files)]\n",
    "map_key_to_index = {future.key: n for n, future in enumerate(futures)}\n",
    "errors = [None] * len(files)\n",
    "outcome = [future.status for future in futures]\n",
    "for future, result in as_completed(futures,\n",
    "                                   with_results=True,\n",
    "                                   raise_errors=False):\n",
    "    idx = map_key_to_index[future.key]\n",
    "    outcome[idx] = future.status\n",
    "    exc = future.exception()\n",
    "    if exc is not None:\n",
    "        errors[idx] = (type(exc), exc)\n",
    "    future.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reclassification.out', 'w') as fd:    \n",
    "    for nt, (out, err, file) in enumerate(zip(outcome,\n",
    "                                              errors,\n",
    "                                              files)):\n",
    "        if err is None:\n",
    "            s = out\n",
    "        else:\n",
    "            s = '{}: {}, {}'.format(out, err[0].__name__, err[1])\n",
    "        fd.write('{:03d} {:30s} {}\\n'.format(nt+1, file.name, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = [f.as_posix() for out, f in zip(outcome, files) if out != 'finished']\n",
    "if failed:\n",
    "    with open('reclassification_failed.json', 'w') as f:\n",
    "        json.dump(failed, f)\n",
    "    raise RuntimeError('Some of the reclassifications have failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
