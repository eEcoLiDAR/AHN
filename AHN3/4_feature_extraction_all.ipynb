{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "def last_modified(opts, remote_path):\n",
    "    info = get_info_remote(get_wdclient(opts), remote_path.as_posix())\n",
    "    format_ = '%a, %d %b %Y %H:%M:%S GMT'\n",
    "    return datetime.datetime.strptime(info['modified'], format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline AHN3 Workflow - Feature Extraction (All Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    "Fill in the username/password for the SURF dCache. Choose whether you want to i) run all input files, ii) run the only input files listed in `filename`, or iii) run the input that was updated since the last workflow run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_path_root = pathlib.Path('/pnfs/grid.sara.nl/data/projects.nl/eecolidar/01_Escience/')\n",
    "\n",
    "# dCache path to retiled data \n",
    "remote_path_input = remote_path_root / 'ALS/Netherlands/ahn3_current/ahn3_current_TOP10NL_ud20200323_normalized'\n",
    "\n",
    "# dCache path where to copy the targets\n",
    "remote_path_output = remote_path_input.with_name(remote_path_input.name.replace('normalized', 'targets_all'))\n",
    "\n",
    "run = 'from_file' # 'all', 'updated', 'from_file'\n",
    "filename = 'feature_extraction_all_failed.json'  # if run is 'from_file', set name of file with input file names\n",
    "assert run in ['all', 'updated', 'from_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WebDAV username:  fnattin4\n",
      "WebDAV password:  ············\n"
     ]
    }
   ],
   "source": [
    "webdav_login = input('WebDAV username: ')\n",
    "webdav_password = getpass.getpass('WebDAV password: ')\n",
    "if run == 'updated':\n",
    "    last_run = datetime.datetime.strptime(input('Date last run (YYYY-MM-DD): '), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Connection to Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_opts = {\n",
    "    'webdav_hostname': 'https://webdav.grid.surfsara.nl:2880',\n",
    "    'webdav_login': webdav_login,\n",
    "    'webdav_password': webdav_password,\n",
    "    'webdav_timeout': 200\n",
    "}\n",
    "assert get_wdclient(wd_opts).check(remote_path_root.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 37457 tiles\n",
      "Retrieve and extract features for: 4 tiles\n"
     ]
    }
   ],
   "source": [
    "tiles = [t for t in list_remote(get_wdclient(wd_opts), remote_path_input.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*.laz')]\n",
    "print('Found: {} tiles'.format(len(tiles)))\n",
    "if run == 'updated':\n",
    "    # determine which tiles have been updated since last run\n",
    "    tiles = [t for t in tiles if last_modified(wd_opts, remote_path_input/t) > last_run]\n",
    "elif run == 'from_file':\n",
    "    with open(filename, 'r') as f:\n",
    "        tiles_read = json.load(f)\n",
    "    # check whether all files are available on dCache\n",
    "    assert all([t in tiles for t in tiles_read]), f'Some of the files in {filename} are not in remote dir'\n",
    "    tiles = tiles_read\n",
    "print('Retrieve and extract features for: {} tiles'.format(len(tiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - -----------------------------------------------\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at: tcp://145.100.59.123:8786\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.59:35933'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.59:44675'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.118:39845'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.118:35757'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.123:37715'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.123:33353'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.115:33933'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.115:46663'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.182:42829'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.84:44931'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.182:42859'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.197:35939'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.84:45233'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.197:34645'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.27:42515'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.48:45511'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.27:41893'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.187:45843'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.48:40761'\n",
      "distributed.deploy.ssh - INFO - distributed.nanny - INFO -         Start Nanny at: 'tcp://145.100.59.187:42493'\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.59:36089\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.115:32897\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.118:34557\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.123:44737\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.84:38135\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.182:44707\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.197:44787\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.27:33941\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:  tcp://145.100.59.48:33117\n",
      "distributed.deploy.ssh - INFO - /usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "distributed.deploy.ssh - INFO - warnings.warn(msg)\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at: tcp://145.100.59.187:39627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>SSHCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='/proxy/8787/status' target='_blank'>/proxy/8787/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "SSHCluster('tcp://145.100.59.123:8786', workers=20, threads=20, memory=337.31 GB)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "local_tmp = pathlib.Path('/data/local/tmp')\n",
    "\n",
    "nprocs_per_node = 2  \n",
    "\n",
    "# start the cluster\n",
    "scheduler_node = 'node1'\n",
    "\n",
    "hosts = [f'node{i}' for i in range(1, 11)]\n",
    "cluster = SSHCluster(hosts=[scheduler_node] + hosts, \n",
    "                     connect_options={'known_hosts': None, \n",
    "                                      'username': 'ubuntu', \n",
    "                                      'client_keys': '/home/ubuntu/.ssh/id_rsa'},\n",
    "                     worker_options={'nthreads': 1, \n",
    "                                     'nprocs': nprocs_per_node,\n",
    "                                     'memory_limit': 'auto',\n",
    "                                     'local_directory': local_tmp/'dask-worker-space'}, \n",
    "                     scheduler_options={'dashboard_address': '8787'})\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We extract features for all points available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details of the retiling schema\n",
    "grid = {\n",
    "    'min_x': -113107.81,\n",
    "    'max_x': 398892.19,\n",
    "    'min_y': 214783.87,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 512\n",
    "}\n",
    "\n",
    "# target mesh size & list of features\n",
    "tile_mesh_size = 10.\n",
    "features = ['density_absolute_mean_normalized_height', 'pulse_penetration_ratio', 'point_density']\n",
    "\n",
    "# setup input dictionary to configure the feature extraction pipeline\n",
    "feature_extraction_input_all = {\n",
    "    #######\n",
    "    'log_config': {'level': 'DEBUG'},\n",
    "    #######\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_input.as_posix(),\n",
    "    'load': {'attributes': ['raw_classification', 'normalized_height']},\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'clear_cache' : {},\n",
    "    'pushremote': remote_path_output.as_posix(),\n",
    "    'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('feature_extraction_all.json', 'w') as f:\n",
    "    json.dump(feature_extraction_input_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# extract the tile indices from the tile names\n",
    "tile_indices = [[int(el) for el in tile.split('.')[0].split('_')[1:]] for tile in tiles]\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [DataProcessing(t, tile_index=idx).config(feature_extraction_input_all).setup_webdav_client(wd_opts) \n",
    "               for t, idx in zip(tiles, tile_indices)]\n",
    "macro.set_labels([os.path.splitext(tile)[0] for tile in tiles])\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and write name of failed pipelines to file\n",
    "macro.print_outcome(to_file='feature_extraction_all.out')\n",
    "failed = macro.get_failed_pipelines()\n",
    "if failed:\n",
    "    with open('feature_extraction_all_failed.json', 'w') as f:\n",
    "        json.dump(['.'.join([pip.label, 'laz']) for pip in failed], f)\n",
    "    raise RuntimeError('Some of the pipelines have failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "macro.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
